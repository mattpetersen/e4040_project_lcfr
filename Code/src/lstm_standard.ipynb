{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard LSTM\n",
    "## Our standard LSTM is not actually Traditional LSTM but Peephole LSTM. More on this you can find in our report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that our LSTM model takes experiment as parameter such as experiment = {'standard', 'moving_average', 'deep'}. Also, nhidden2 (number of units on layer 2) activates only if you choose experiment='deep'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since our paper uses for train data 4978 sentences with 56590 words and for test data 893 sentences with 9198 words, we had to combine our train and validation sets in order to use the same data and try to match the same results with the paper. Note that since we have ran it before with validation sets we would expect our model to converge before 40 epochs. Thus, for our experiments we would usually use n <= 40 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN Version is too old. Update to v5, was 3007.)\n",
      "/home/ubuntu/miniconda2/envs/theano/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose: True\n",
      "win: 3\n",
      "layer_norm: False\n",
      "seed: 345\n",
      "moving_avg: 3\n",
      "nhidden: 300\n",
      "decay: True\n",
      "experiment: standard\n",
      "lr: 0.1\n",
      "folder: ../result\n",
      "minibatch_size: 4978\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "nepochs: 15\n",
      "simplified_type: no_forget\n",
      "emb_dimension: 40\n",
      "savemodel: False\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 83.190\n",
      "NEW BEST: epoch 1, minibatch 1/1, best test F1: 88.210\n",
      "NEW BEST: epoch 2, minibatch 1/1, best test F1: 91.110\n",
      "NEW BEST: epoch 3, minibatch 1/1, best test F1: 91.730\n",
      "NEW BEST: epoch 4, minibatch 1/1, best test F1: 92.320\n",
      "\n",
      "NEW BEST: epoch 6, minibatch 1/1, best test F1: 93.260\n",
      "\n",
      "\n",
      "\n",
      "NEW BEST: epoch 10, minibatch 1/1, best test F1: 93.370\n",
      "\n",
      "\n",
      "NEW BEST: epoch 13, minibatch 1/1, best test F1: 93.630\n",
      "\n",
      "('BEST RESULT: epoch', 13, 'best test F1', 93.63, 'with the model', '../result')\n",
      "verbose: True\n",
      "win: 3\n",
      "layer_norm: False\n",
      "seed: 345\n",
      "moving_avg: 3\n",
      "nhidden: 300\n",
      "decay: True\n",
      "experiment: standard\n",
      "lr: 0.1\n",
      "folder: ../result\n",
      "minibatch_size: 4978\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "nepochs: 15\n",
      "simplified_type: no_forget\n",
      "emb_dimension: 60\n",
      "savemodel: False\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 84.520\n",
      "NEW BEST: epoch 1, minibatch 1/1, best test F1: 89.610\n",
      "NEW BEST: epoch 2, minibatch 1/1, best test F1: 91.590\n",
      "NEW BEST: epoch 3, minibatch 1/1, best test F1: 92.280\n",
      "NEW BEST: epoch 4, minibatch 1/1, best test F1: 92.810\n",
      "NEW BEST: epoch 5, minibatch 1/1, best test F1: 92.820\n",
      "NEW BEST: epoch 6, minibatch 1/1, best test F1: 93.750\n",
      "\n",
      "NEW BEST: epoch 8, minibatch 1/1, best test F1: 93.780\n",
      "\n",
      "NEW BEST: epoch 10, minibatch 1/1, best test F1: 94.070\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('BEST RESULT: epoch', 10, 'best test F1', 94.07, 'with the model', '../result')\n",
      "verbose: True\n",
      "win: 3\n",
      "layer_norm: False\n",
      "seed: 345\n",
      "moving_avg: 3\n",
      "nhidden: 300\n",
      "decay: True\n",
      "experiment: standard\n",
      "lr: 0.1\n",
      "folder: ../result\n",
      "minibatch_size: 4978\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "nepochs: 15\n",
      "simplified_type: no_forget\n",
      "emb_dimension: 70\n",
      "savemodel: False\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 85.870\n",
      "NEW BEST: epoch 1, minibatch 1/1, best test F1: 90.250\n",
      "NEW BEST: epoch 2, minibatch 1/1, best test F1: 91.580\n",
      "NEW BEST: epoch 3, minibatch 1/1, best test F1: 92.960\n",
      "NEW BEST: epoch 4, minibatch 1/1, best test F1: 93.710\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NEW BEST: epoch 9, minibatch 1/1, best test F1: 93.860\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('BEST RESULT: epoch', 9, 'best test F1', 93.86, 'with the model', '../result')\n",
      "verbose: True\n",
      "win: 3\n",
      "layer_norm: False\n",
      "seed: 345\n",
      "moving_avg: 3\n",
      "nhidden: 300\n",
      "decay: True\n",
      "experiment: standard\n",
      "lr: 0.1\n",
      "folder: ../result\n",
      "minibatch_size: 4978\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "nepochs: 15\n",
      "simplified_type: no_forget\n",
      "emb_dimension: 80\n",
      "savemodel: False\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 85.090\n",
      "NEW BEST: epoch 1, minibatch 1/1, best test F1: 89.980\n",
      "NEW BEST: epoch 2, minibatch 1/1, best test F1: 91.590\n",
      "NEW BEST: epoch 3, minibatch 1/1, best test F1: 92.260\n",
      "NEW BEST: epoch 4, minibatch 1/1, best test F1: 92.640\n",
      "NEW BEST: epoch 5, minibatch 1/1, best test F1: 92.660\n",
      "NEW BEST: epoch 6, minibatch 1/1, best test F1: 93.550\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('BEST RESULT: epoch', 6, 'best test F1', 93.55, 'with the model', '../result')\n",
      "verbose: True\n",
      "win: 3\n",
      "layer_norm: False\n",
      "seed: 345\n",
      "moving_avg: 3\n",
      "nhidden: 300\n",
      "decay: True\n",
      "experiment: standard\n",
      "lr: 0.1\n",
      "folder: ../result\n",
      "minibatch_size: 4978\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "nepochs: 15\n",
      "simplified_type: no_forget\n",
      "emb_dimension: 90\n",
      "savemodel: False\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 85.490\n",
      "NEW BEST: epoch 1, minibatch 1/1, best test F1: 89.350\n",
      "NEW BEST: epoch 2, minibatch 1/1, best test F1: 91.920\n",
      "NEW BEST: epoch 3, minibatch 1/1, best test F1: 92.710\n",
      "NEW BEST: epoch 4, minibatch 1/1, best test F1: 93.010\n",
      "NEW BEST: epoch 5, minibatch 1/1, best test F1: 93.150\n",
      "NEW BEST: epoch 6, minibatch 1/1, best test F1: 93.290\n",
      "\n",
      "NEW BEST: epoch 8, minibatch 1/1, best test F1: 93.840\n",
      "NEW BEST: epoch 9, minibatch 1/1, best test F1: 94.480\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('BEST RESULT: epoch', 9, 'best test F1', 94.48, 'with the model', '../result')\n",
      "verbose: True\n",
      "win: 3\n",
      "layer_norm: False\n",
      "seed: 345\n",
      "moving_avg: 3\n",
      "nhidden: 300\n",
      "decay: True\n",
      "experiment: standard\n",
      "lr: 0.1\n",
      "folder: ../result\n",
      "minibatch_size: 4978\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "nepochs: 15\n",
      "simplified_type: no_forget\n",
      "emb_dimension: 100\n",
      "savemodel: False\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 85.280\n",
      "NEW BEST: epoch 1, minibatch 1/1, best test F1: 89.100\n",
      "NEW BEST: epoch 2, minibatch 1/1, best test F1: 91.750\n",
      "NEW BEST: epoch 3, minibatch 1/1, best test F1: 92.800\n",
      "NEW BEST: epoch 4, minibatch 1/1, best test F1: 93.130\n",
      "NEW BEST: epoch 5, minibatch 1/1, best test F1: 93.580\n",
      "\n",
      "\n",
      "NEW BEST: epoch 8, minibatch 1/1, best test F1: 93.690\n",
      "NEW BEST: epoch 9, minibatch 1/1, best test F1: 94.290\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('BEST RESULT: epoch', 9, 'best test F1', 94.29, 'with the model', '../result')\n",
      "verbose: True\n",
      "win: 3\n",
      "layer_norm: False\n",
      "seed: 345\n",
      "moving_avg: 3\n",
      "nhidden: 300\n",
      "decay: True\n",
      "experiment: standard\n",
      "lr: 0.1\n",
      "folder: ../result\n",
      "minibatch_size: 4978\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "nepochs: 15\n",
      "simplified_type: no_forget\n",
      "emb_dimension: 110\n",
      "savemodel: False\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 85.520\n",
      "NEW BEST: epoch 1, minibatch 1/1, best test F1: 89.750\n",
      "NEW BEST: epoch 2, minibatch 1/1, best test F1: 92.120\n",
      "\n",
      "NEW BEST: epoch 4, minibatch 1/1, best test F1: 93.050\n",
      "\n",
      "NEW BEST: epoch 6, minibatch 1/1, best test F1: 93.400\n",
      "\n",
      "NEW BEST: epoch 8, minibatch 1/1, best test F1: 93.540\n",
      "NEW BEST: epoch 9, minibatch 1/1, best test F1: 93.740\n",
      "\n",
      "\n",
      "NEW BEST: epoch 12, minibatch 1/1, best test F1: 93.760\n",
      "\n",
      "\n",
      "('BEST RESULT: epoch', 12, 'best test F1', 93.76, 'with the model', '../result')\n",
      "verbose: True\n",
      "win: 3\n",
      "layer_norm: False\n",
      "seed: 345\n",
      "moving_avg: 3\n",
      "nhidden: 300\n",
      "decay: True\n",
      "experiment: standard\n",
      "lr: 0.1\n",
      "folder: ../result\n",
      "minibatch_size: 4978\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "nepochs: 15\n",
      "simplified_type: no_forget\n",
      "emb_dimension: 120\n",
      "savemodel: False\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 87.010\n",
      "NEW BEST: epoch 1, minibatch 1/1, best test F1: 89.750\n",
      "NEW BEST: epoch 2, minibatch 1/1, best test F1: 91.990\n",
      "NEW BEST: epoch 3, minibatch 1/1, best test F1: 92.670\n",
      "NEW BEST: epoch 4, minibatch 1/1, best test F1: 93.360\n",
      "NEW BEST: epoch 5, minibatch 1/1, best test F1: 93.450\n",
      "\n",
      "NEW BEST: epoch 7, minibatch 1/1, best test F1: 93.790\n",
      "NEW BEST: epoch 8, minibatch 1/1, best test F1: 93.810\n",
      "NEW BEST: epoch 9, minibatch 1/1, best test F1: 93.970\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('BEST RESULT: epoch', 9, 'best test F1', 93.97, 'with the model', '../result')\n",
      "verbose: True\n",
      "win: 3\n",
      "layer_norm: False\n",
      "seed: 345\n",
      "moving_avg: 3\n",
      "nhidden: 300\n",
      "decay: True\n",
      "experiment: standard\n",
      "lr: 0.1\n",
      "folder: ../result\n",
      "minibatch_size: 4978\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "nepochs: 15\n",
      "simplified_type: no_forget\n",
      "emb_dimension: 130\n",
      "savemodel: False\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 85.400\n",
      "NEW BEST: epoch 1, minibatch 1/1, best test F1: 90.250\n",
      "NEW BEST: epoch 2, minibatch 1/1, best test F1: 91.520\n",
      "NEW BEST: epoch 3, minibatch 1/1, best test F1: 92.680\n",
      "NEW BEST: epoch 4, minibatch 1/1, best test F1: 93.310\n",
      "\n",
      "NEW BEST: epoch 6, minibatch 1/1, best test F1: 93.790\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NEW BEST: epoch 12, minibatch 1/1, best test F1: 93.830\n",
      "\n",
      "\n",
      "('BEST RESULT: epoch', 12, 'best test F1', 93.83, 'with the model', '../result')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"First we do our grid search and try to come as close as to the results on paper for simple LSTM to identify \n",
    "embedding dimensions since they were not mentioned on paper. After running around 10 experiments we conclude that \n",
    "when emb_dimension = 90 we had the highest result of 94.29. However, this is still a bit lower than acqired result of \n",
    "94.85 by paper. So for next one we will use Adam optimizer to see if we can achieve closer result.\"\"\"\n",
    "\n",
    "import lstm\n",
    "import imp\n",
    "imp.reload(lstm)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    if i is 1:\n",
    "        continue\n",
    "    param = {\n",
    "        'experiment':'standard',\n",
    "        'lr': 0.1,\n",
    "        'verbose': True,\n",
    "        'decay': True,\n",
    "        'win': 3,\n",
    "        'nhidden': 300,\n",
    "        'nhidden2':300,\n",
    "        'seed': 345,\n",
    "        'emb_dimension': 40+i*10,\n",
    "        'nepochs': 15,\n",
    "        'savemodel': False,\n",
    "        'normal': True,\n",
    "        'layer_norm': False,\n",
    "        'minibatch_size':4978,\n",
    "        'moving_avg':3,\n",
    "        'simplified_type':'no_forget',\n",
    "        'folder':'../result_standard'}\n",
    "\n",
    "    lstm.test_lstm(**param);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose: True\n",
      "win: 3\n",
      "layer_norm: False\n",
      "seed: 345\n",
      "minibatch_size: 4978\n",
      "nhidden: 300\n",
      "decay: True\n",
      "with_attention: False\n",
      "experiment: standard\n",
      "lr: 0.1\n",
      "folder: ../result_standard\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "nepochs: 35\n",
      "simplified_type: no_forget\n",
      "emb_dimension: 90\n",
      "savemodel: False\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 91.550\n",
      "NEW BEST: epoch 1, minibatch 1/1, best test F1: 92.990\n",
      "\n",
      "NEW BEST: epoch 3, minibatch 1/1, best test F1: 93.890\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NEW BEST: epoch 8, minibatch 1/1, best test F1: 93.990\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('Decay happened. New Learning Rate:', 0.05)\n",
      "NEW BEST: epoch 19, minibatch 1/1, best test F1: 94.260\n",
      "NEW BEST: epoch 20, minibatch 1/1, best test F1: 94.520\n",
      "\n",
      "\n",
      "NEW BEST: epoch 23, minibatch 1/1, best test F1: 94.610\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('Decay happened. New Learning Rate:', 0.025)\n",
      "NEW BEST: epoch 34, minibatch 1/1, best test F1: 94.640\n",
      "('BEST RESULT: epoch', 34, 'best test F1', 94.64, 'with the model', '../result_standard')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"After running with Adam optimizer and keeping everything same (Note that for Adam we are deviding lr by 100 when \n",
    "we are inputting it to Adam as learning rate) we got 94.64 in 35 epochs. Since, we reached it at 34th epoch and it did\n",
    "not look like it has converged we might try to increase the number of epochs and get even better result.\"\"\"\n",
    "\n",
    "import lstm\n",
    "import imp\n",
    "imp.reload(lstm)\n",
    "\n",
    "param = {\n",
    "    'experiment':'standard',\n",
    "    'lr': 0.1,\n",
    "    'verbose': True,\n",
    "    'decay': True,\n",
    "    'win': 3,\n",
    "    'nhidden': 300,\n",
    "    'nhidden2':300,\n",
    "    'seed': 345,\n",
    "    'emb_dimension': 90,\n",
    "    'nepochs': 35,\n",
    "    'savemodel': False,\n",
    "    'normal': True,\n",
    "    'layer_norm': False,\n",
    "    'minibatch_size':4978,\n",
    "    'with_attention':False,\n",
    "    'simplified_type':'no_forget',\n",
    "    'folder':'../result_standard'}\n",
    "\n",
    "lstm.test_lstm(**param);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
